{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DetermineOptimalThreshold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPrdgdbp66InR2wLZb5gdAN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lustea0201/Biomedical-Data-Science/blob/master/DetermineOptimalThreshold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJWLB_GHkY_1",
        "colab_type": "text"
      },
      "source": [
        "# Interpreting the outputs of the model on the second dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy5jZvelpXpl",
        "colab_type": "code",
        "outputId": "ac3b14ce-2b79-46d6-d5d2-229708467ac6",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "source": [
        "\n",
        "#@title\n",
        "import numpy as np\n",
        "import torch \n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!pip install flashtorch\n",
        "from flashtorch.saliency import Backprop\n",
        "\n",
        "!pip install shap \n",
        "import shap\n",
        "\n",
        "!pip install pytorch-gradcam\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid, save_image\n",
        "\n",
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAM, GradCAMpp\n",
        "\n",
        "!wget https://raw.githubusercontent.com/yiskw713/SmoothGradCAMplusplus/master/cam.py -P local_modules -nc\n",
        "!wget https://raw.githubusercontent.com/yiskw713/SmoothGradCAMplusplus/master/utils/visualize.py -P local_modules -nc\n",
        "import sys\n",
        "sys.path.append('local_modules')\n",
        "from PIL import Image\n",
        "from local_modules.visualize import visualize\n",
        "import local_modules.cam as smooth\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/HierarchicalShapley.py -P local_modules -nc\n",
        "import local_modules.HierarchicalShapley as HS\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/utils.py -P local_modules -nc\n",
        "import local_modules.utils as utils\n",
        "\n",
        "!wget https://raw.githubusercontent.com/lustea0201/Interpretability/master/interpret11.py -P local_modules -nc\n",
        "import local_modules.interpret11 as intp\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "torch.manual_seed(0)\n",
        "dtype = torch.float\n",
        "\n",
        "\n",
        "\n",
        "net = utils.HdNet()\n",
        "# Loading the trained dictionnary state\n",
        "\n",
        "net.load_state_dict(torch.load('drive/My Drive/Interpretability/modelHD.pth')) \n",
        "\n",
        "data = zipfile.ZipFile(\"/content/drive/My Drive/Interpretability/HD/data2/data.zip\", 'r')\n",
        "\n",
        "root_dir = \"main_dir\"\n",
        "data.extractall(root_dir)\n",
        "data.close()\n",
        "\n",
        "Batch_Size = 64\n",
        "MEAN, STD = np.array([0.5, 0.5, 0.5]), np.array([0.5, 0.5, 0.5])\n",
        "\n",
        "transf = transforms.Compose( [ToTensor(), Normalize(mean=MEAN, std=STD)])\n",
        "\n",
        "train_data = ImageFolder(root = os.path.join(root_dir, 'train'), transform = transf)\n",
        "dataloader = DataLoader(train_data, batch_size = Batch_Size, shuffle = True, num_workers = 0)\n",
        "train_loader = iter(dataloader)\n",
        "\n",
        "# For SHAP\n",
        "X,Y = next(train_loader)\n",
        "bg_choice = None # Default: entire training set, \"average\", \"white\", \"black\" \n",
        "# Median would result in white \n",
        "background = X\n",
        "if (bg_choice == \"black\"): \n",
        "  background = -torch.ones(X.shape)\n",
        "elif (bg_choice == \"white\"): \n",
        "  background = torch.ones(X.shape)\n",
        "elif (bg_choice == \"average\"): \n",
        "  avg = torch.mean(X, dim = 0)\n",
        "  display_image(avg, 0)\n",
        "\n",
        "e = shap.GradientExplainer(net, background)\n",
        "\n",
        "# For Hierarchical   \n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class HierarchicalShap:\n",
        "    \"\"\"\n",
        "    Explains the salient regions of images according a given network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, background, mean=np.array([0.5, 0.5, 0.5]), sd=np.array([0.5, 0.5, 0.5])):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        model : the model from which you wish to study the decision\n",
        "        background : used to remove the contribution of non-considered regions when constructing subsets\n",
        "        mean : the mean used for image normalization (useful for plotting from input)\n",
        "        sd : the standard deviation used for normalization (useful for plotting from input)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.background = background\n",
        "        self.mean = mean\n",
        "        self.sd = sd\n",
        "\n",
        "    def display_cropped_images(self, images, scores):\n",
        "        \"\"\"\n",
        "        Draw the subsets.\n",
        "        Parameters\n",
        "        ----------\n",
        "        images : all the subsets to draw\n",
        "        scores : the output score for a class 1\n",
        "        \"\"\"\n",
        "        fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                im = images[4 * i + j].numpy().transpose(1, 2, 0)\n",
        "                im = im * self.sd + self.mean\n",
        "                axs[i, j].imshow(im)\n",
        "                axs[i, j].set_title(\"#%d score:%f \" % (4 * i + j, scores[4 * i + j]))\n",
        "\n",
        "    def construct_subsets(self, im, s=(0, 0), region_size=(None, None)):\n",
        "        \"\"\"\n",
        "        Construct the subsets of im: all possible image resulting from removing from im the content of 0, 1, 2, 3\n",
        "        or all 4 quadrants of the region defined by start and region_size .\n",
        "        Parameters\n",
        "        ----------\n",
        "        im : the image from which to extract subsets\n",
        "        s : the top left pixel coordinates of the region analyzed, a tuple of\n",
        "        region_size : the size of the region analyzed\n",
        "        Returns\n",
        "        --------\n",
        "        subsets : the list of 16 images\n",
        "        r_coord : a 2x2 array where each entry is a tuple of tuples; the first indicating the start\n",
        "                  of the region and the second its size\n",
        "        \"\"\"\n",
        "\n",
        "        m = (s[0] + region_size[0] // 2, s[1] + region_size[1] // 2)\n",
        "        e = (s[0] + region_size[0], s[1] + region_size[1])\n",
        "\n",
        "        top_left = (s, (m[0] - s[0], m[1] - s[1]))\n",
        "        top_right = ((s[0], m[1]), (m[0] - s[0], e[1] - m[1]))\n",
        "        bottom_left = ((m[0], s[1]), (e[0] - m[0], m[1] - s[1]))\n",
        "        bottom_right = (m, (e[0] - m[0], e[1] - m[1]))\n",
        "        r_coord = np.array([[top_left, top_right], [bottom_left, bottom_right]])\n",
        "\n",
        "        subsets_size = [16, im.shape[0], im.shape[1], im.shape[2]]\n",
        "\n",
        "        bg = self.background\n",
        "        # removing 0 features\n",
        "        im1234 = bg.clone()\n",
        "        im1234[:, s[0]:e[0], s[1]:e[1]] = im[:, s[0]:e[0], s[1]:e[1]]\n",
        "        # removing 1 feature\n",
        "        im234 = im1234.clone()\n",
        "        im234[:, s[0]:m[0], s[1]:m[1]] = bg[:, s[0]:m[0], s[1]:m[1]]\n",
        "        im134 = im1234.clone()\n",
        "        im134[:, s[0]:m[0], m[1]:e[1]] = bg[:, s[0]:m[0], m[1]:e[1]]\n",
        "        im124 = im1234.clone()\n",
        "        im124[:, m[0]:e[0], s[1]:m[1]] = bg[:, m[0]:e[0], s[1]:m[1]]\n",
        "        im123 = im1234.clone()\n",
        "        im123[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        # removing 2 features\n",
        "        im34 = im234.clone()\n",
        "        im34[:, s[0]:m[0], m[1]:e[1]] = bg[:, s[0]:m[0], m[1]:e[1]]\n",
        "        im24 = im234.clone()\n",
        "        im24[:, m[0]:e[0], s[1]:m[1]] = bg[:, m[0]:e[0], s[1]:m[1]]\n",
        "        im23 = im234.clone()\n",
        "        im23[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        im14 = im134.clone()\n",
        "        im14[:, m[0]:e[0], s[1]:m[1]] = bg[:, m[0]:e[0], s[1]:m[1]]\n",
        "        im13 = im134.clone()\n",
        "        im13[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        im12 = im123.clone()\n",
        "        im12[:, m[0]:e[0], s[1]:m[1]] = bg[:, m[0]:e[0], s[1]:m[1]]\n",
        "        # removing 3 features\n",
        "        im4 = im34.clone()\n",
        "        im4[:, m[0]:e[0], s[1]:m[1]] = bg[:, m[0]:e[0], s[1]:m[1]]\n",
        "        im3 = im34.clone()\n",
        "        im3[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        im2 = im24.clone()\n",
        "        im2[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        im1 = im14.clone()\n",
        "        im1[:, m[0]:e[0], m[1]:e[1]] = bg[:, m[0]:e[0], m[1]:e[1]]\n",
        "        # removing 4\n",
        "        im_ = bg.clone()\n",
        "\n",
        "        subsets = torch.zeros(size=subsets_size)\n",
        "        subsets[0] = im1234\n",
        "        subsets[1] = im234\n",
        "        subsets[2] = im134\n",
        "        subsets[3] = im124\n",
        "        subsets[4] = im123\n",
        "        subsets[5] = im34\n",
        "        subsets[6] = im24\n",
        "        subsets[7] = im23\n",
        "        subsets[8] = im14\n",
        "        subsets[9] = im13\n",
        "        subsets[10] = im12\n",
        "        subsets[11] = im4\n",
        "        subsets[12] = im3\n",
        "        subsets[13] = im2\n",
        "        subsets[14] = im1\n",
        "        subsets[15] = im_\n",
        "\n",
        "        return subsets, r_coord\n",
        "\n",
        "    def subset_scores(self, sub, label):\n",
        "        \"\"\"\n",
        "        Compute the scores of each subset input.\n",
        "        Parameters\n",
        "        ----------\n",
        "        sub : the subsets of inputs\n",
        "        label : the class label - typically 1 -  in which we're interested.\n",
        "        Returns\n",
        "        --------\n",
        "        score : an array of the 16 scores for each input\n",
        "        \"\"\"\n",
        "        outputs = self.model(sub)\n",
        "        score = outputs[:, label].detach().numpy()\n",
        "\n",
        "        return score\n",
        "\n",
        "    def shapley_of_quadrants(self, score):\n",
        "        \"\"\"\n",
        "        Return a 2x2 array which contains the Shapley values associated with each quadrant\n",
        "        Parameters\n",
        "        ----------\n",
        "        score : the network evaluation for each subset\n",
        "        Returns\n",
        "        --------\n",
        "        shapley_coefficients : an array of the 16 scores for each input\n",
        "        \"\"\"\n",
        "\n",
        "        phi1 = (score[14] - score[15] + score[0] - score[1]) / 4\\\n",
        "               + (score[8] - score[11] + score[9] - score[12] + score[10] - score[13]\n",
        "                  + score[2] - score[5] + score[3] - score[6] + score[4] - score[7]) / 12\n",
        "\n",
        "        phi2 = (score[13] - score[15] + score[0] - score[2]) / 4 \\\n",
        "               + (score[6] - score[11] + score[7] - score[12] + score[10] - score[14]\n",
        "                  + score[1] - score[5] + score[3] - score[8] + score[4] - score[9]) / 12\n",
        "\n",
        "        phi3 = (score[12] - score[15] + score[0] - score[3]) / 4 \\\n",
        "               + (score[5] - score[11] + score[9] - score[14] + score[7] - score[13]\n",
        "                  + score[2] - score[8] + score[1] - score[6] + score[4] - score[10]) / 12\n",
        "\n",
        "        phi4 = (score[11] - score[15] + score[0] - score[4]) / 4 \\\n",
        "               + (score[8] - score[14] + score[5] - score[12] + score[6] - score[13]\n",
        "                  + score[1] - score[7] + score[3] - score[10] + score[2] - score[9]) / 12\n",
        "\n",
        "        shapley_coefficients = np.array([[phi1, phi2], [phi3, phi4]])\n",
        "        return shapley_coefficients\n",
        "\n",
        "    def get_salient_regions(self, shapley_values, tol, regions):\n",
        "        \"\"\"\n",
        "        Determine which of the 4 quadrants are salient, i.e. have Shapley value larger than tol\n",
        "        Parameters\n",
        "        ----------\n",
        "        shapley_values : the Shapley coefficients associated with each quadrant\n",
        "        tol : the specified tolerance for a sub-region to be considered salient\n",
        "        regions : the coordinates associated with each quadrant\n",
        "        Returns\n",
        "        --------\n",
        "        srs : a list of the coordinates of the quadrants whose Shapley values were large enough\n",
        "        \"\"\"\n",
        "        srs = []\n",
        "        for i in range(len(shapley_values)):\n",
        "            for j in range(len(shapley_values[0])):\n",
        "                if shapley_values[i, j] > tol:\n",
        "                    srs.append(regions[i, j])\n",
        "\n",
        "        return srs\n",
        "\n",
        "    def display_salient(self, im, srs_coll, count, filename):\n",
        "        \"\"\"\n",
        "        Determine which of the 4 quadrants are salient, i.e. have Shapley value larger than tol\n",
        "        Parameters\n",
        "        ----------\n",
        "        im : the original image, in input format\n",
        "        srs_coll : a collection of all regions deemed salient\n",
        "        count : a normalizing mask which determines how many time each pixel was given a chance to be salient\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(60, 30))\n",
        "\n",
        "        sample_image = im.numpy().transpose(1, 2, 0)\n",
        "        count = count.transpose(1, 2, 0)\n",
        "        ax4.imshow(count / np.max(count))\n",
        "        image = sample_image * self.sd + self.mean\n",
        "        ax1.imshow(image)\n",
        "        ax2.imshow(image)\n",
        "        mask = np.zeros(image.shape)\n",
        "\n",
        "        # Count how many time each pixel was found to be in a salient region\n",
        "        for srs in srs_coll:\n",
        "            for sr in srs:\n",
        "                start = sr[0]\n",
        "                q_size = sr[1]\n",
        "                xs = [start[1], start[1] + q_size[1], start[1] + q_size[1], start[1]]\n",
        "                ys = [start[0], start[0], start[0] + q_size[0], start[0] + q_size[0]]\n",
        "                ax2.fill(xs, ys, 'r', alpha=1 / len(srs_coll))\n",
        "\n",
        "                mask[start[0]:start[0] + q_size[0], start[1]:start[1] + q_size[1], :] += np.ones(\n",
        "                    (q_size[0], q_size[1], 3))\n",
        "\n",
        "        # Normalize the mask by the number of tries in each region\n",
        "        mask /= count\n",
        "        # Normalize the mask to the range (0,1)\n",
        "        mask /= np.max(mask)\n",
        "        # Set to 0 elements smaller than 1/5\n",
        "        negligible = (mask < 1 / 5)\n",
        "        mask[negligible] = 0\n",
        "\n",
        "        ax1.set_xlim([0, im.shape[2]])\n",
        "        ax1.set_ylim([im.shape[1], 0])\n",
        "        ax2.set_xlim([0, im.shape[2]])\n",
        "        ax2.set_ylim([im.shape[1], 0])\n",
        "        ax3.imshow(image * mask)\n",
        "        if filename != None:\n",
        "            plt.savefig(filename, dpi=300)\n",
        "        return mask\n",
        "\n",
        "    def do_all(self, im, label, start, region_size, tol, debug=False):\n",
        "        \"\"\"\n",
        "        Secondary main loop: do everything for one region of the image.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        start : the starting coordinates of the region\n",
        "        region_size : self-explanatory\n",
        "        tol : the specified tolerance for a sub-region to be considered salient\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        Returns\n",
        "        --------\n",
        "        srs : a list of the coordinates of the quadrants whose Shapley values were large enough\n",
        "        \"\"\"\n",
        "        images_final, regions = self.construct_subsets(im, start, region_size)\n",
        "        score = self.subset_scores(images_final, label)\n",
        "        sm = self.shapley_of_quadrants(score)\n",
        "        if debug:\n",
        "            self.display_cropped_images(images_final, score)\n",
        "            f = plt.figure()\n",
        "            sns.heatmap(sm)\n",
        "            f.suptitle(\"Shap values of each quadrant\")\n",
        "\n",
        "        srs = self.get_salient_regions(sm, tol, regions)\n",
        "\n",
        "        return srs\n",
        "\n",
        "    def saliency_map(self, image, label, tolerance, only_one_run=False, debug=False, max_depth=30, filename=None):\n",
        "        \"\"\"\n",
        "        Create and then show a saliency map built with the Hierarchical Shapley method.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        label : the label with respect to which we want to analyze - typically 1\n",
        "        tolerance : the specified tolerance for a sub-region to be considered salient. A list is expected.\n",
        "        only_one_run : when False, several runs are done by also considering 16 cropped versions of the input\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        max_depth : the maximum number of divisions you want to allow before deciding the tolerance is too low.\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        ls = []\n",
        "        count = np.zeros(image.shape)\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        if only_one_run:\n",
        "            starts = [(0, 0)]\n",
        "            ends = [(xf[0], xf[1])]\n",
        "        else:\n",
        "            delta = [image.shape[1] // 20, image.shape[2] // 24]\n",
        "            starts = [(0, 0), (0, delta[1]), (delta[0], 0), (delta[0], delta[1])]\n",
        "            ends = [(xf[0], xf[1]), (xf[0], xf[1] - delta[1]), (xf[0] - delta[0], xf[1]),\n",
        "                    (xf[0] - delta[0], xf[1] - delta[1])]\n",
        "\n",
        "        for start in starts:\n",
        "            for end in ends:\n",
        "                size = (end[0] - start[0], end[1] - start[1])\n",
        "                count[:, start[0]:end[0], start[1]:end[1]] += np.ones((3, size[0], size[1]))\n",
        "\n",
        "        for tol in tolerance:\n",
        "            try:\n",
        "                for start in starts:\n",
        "                    for end in ends:\n",
        "\n",
        "                        size = (end[0] - start[0], end[1] - start[1])\n",
        "                        srs = [(start, size)]\n",
        "                        finished = []\n",
        "                        k = 0\n",
        "\n",
        "                        while len(srs) > 0:\n",
        "\n",
        "                            if k > max_depth:\n",
        "                                raise RuntimeError(\"Depth %d reached at tolerance %f\" % (k, tol))\n",
        "                            all_ = []\n",
        "                            for sr in srs:\n",
        "                                s = self.do_all(image, label, sr[0], sr[1], tol, debug)\n",
        "                                if s == []:\n",
        "                                    finished.append(((sr[0]), (sr[1])))\n",
        "\n",
        "                                else:\n",
        "                                    all_ += s\n",
        "                            srs = all_\n",
        "                            k += 1\n",
        "                        ls.append(finished)\n",
        "            except RuntimeError as w:\n",
        "                print(w, \"Run ignored, consider increasing tolerance.\")\n",
        "\n",
        "        return self.display_salient(image, ls, count, filename)\n",
        "\n",
        "    def get_salient_regions_optim_tol(self, shapley_values, tols, regions):\n",
        "        \"\"\"\n",
        "        Determine which of the 4 quadrants are salient, i.e. have Shapley value larger than tol\n",
        "        Parameters\n",
        "        ----------\n",
        "        shapley_values : the Shapley coefficients associated with each quadrant\n",
        "        tol : the specified tolerance for a sub-region to be considered salient\n",
        "        regions : the coordinates associated with each quadrant\n",
        "        Returns\n",
        "        --------\n",
        "        srs : a list of the coordinates of the quadrants whose Shapley values were large enough\n",
        "        \"\"\"\n",
        "        srs = [[] for r in range(len(tols))]\n",
        "        for i in range(len(shapley_values)):\n",
        "            for j in range(len(shapley_values[0])):\n",
        "                for r in range(len(tols)):\n",
        "                    if shapley_values[i, j] > tols[r]:\n",
        "                        srs[r].append(regions[i, j])\n",
        "        return srs\n",
        "\n",
        "    def do_all_optim_tol(self, im, label, start, region_size, tols, debug=False):\n",
        "        \"\"\"\n",
        "        Secondary main loop: do everything for one region of the image.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        start : the starting coordinates of the region\n",
        "        region_size : self-explanatory\n",
        "        tol : the specified tolerance for a sub-region to be considered salient\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        Returns\n",
        "        --------\n",
        "        srs : a list of the coordinates of the quadrants whose Shapley values were large enough\n",
        "        \"\"\"\n",
        "        images_final, regions = self.construct_subsets(im, start, region_size)\n",
        "        score = self.subset_scores(images_final, label)\n",
        "        sm = self.shapley_of_quadrants(score)\n",
        "        if debug:\n",
        "            self.display_cropped_images(images_final, score)\n",
        "            f = plt.figure()\n",
        "            sns.heatmap(sm)\n",
        "            f.suptitle(\"Shap values of each quadrant\")\n",
        "\n",
        "        srs = self.get_salient_regions_optim_tol(sm, tols, regions)\n",
        "\n",
        "        return srs\n",
        "\n",
        "    def saliency_map_optim_tol(self, image, label, tolerance, only_one_run=False, debug=False, max_depth=30,\n",
        "                               filename=None):\n",
        "        \"\"\"\n",
        "        Create and then show a saliency map built with the Hierarchical Shapley method.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        label : the label with respect to which we want to analyze - typically 1\n",
        "        tolerance : the specified tolerance for a sub-region to be considered salient. A list is expected.\n",
        "        only_one_run : when False, several runs are done by also considering 16 cropped versions of the input\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        max_depth : the maximum number of divisions you want to allow before deciding the tolerance is too low.\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        ls = []\n",
        "        count = np.zeros(image.shape)\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        if only_one_run:\n",
        "            starts = [(0, 0)]\n",
        "            ends = [(xf[0], xf[1])]\n",
        "        else:\n",
        "            delta = [image.shape[1] // 20, image.shape[2] // 24]\n",
        "            starts = [(0, 0), (0, delta[1]), (delta[0], 0), (delta[0], delta[1])]\n",
        "            ends = [(xf[0], xf[1]), (xf[0], xf[1] - delta[1]), (xf[0] - delta[0], xf[1]),\n",
        "                    (xf[0] - delta[0], xf[1] - delta[1])]\n",
        "\n",
        "        for start in starts:\n",
        "            for end in ends:\n",
        "                size = (end[0] - start[0], end[1] - start[1])\n",
        "                count[:, start[0]:end[0], start[1]:end[1]] += np.ones((3, size[0], size[1]))\n",
        "\n",
        "        for start in starts:\n",
        "            for end in ends:\n",
        "\n",
        "                size = (end[0] - start[0], end[1] - start[1])\n",
        "                srs = [[(start, size)] for r in range(len(tolerance))]\n",
        "                finished = [[] for r in range(len(tolerance))]\n",
        "                was_finished = [True for r in range(len(tolerance))]\n",
        "                k = 0\n",
        "\n",
        "                while len(srs[0]) > 0 and k < max_depth:\n",
        "                    all_ = [[] for r in range(len(tolerance))]\n",
        "\n",
        "                    for sr in srs[0]:\n",
        "                        s = self.do_all_optim_tol(image, label, sr[0], sr[1], tolerance, debug)\n",
        "\n",
        "                        for r in range(len(tolerance)):\n",
        "                            if len(srs[r]) > 0:\n",
        "                                if s[r] == []:\n",
        "                                    finished[r].append(((sr[0]), (sr[1])))\n",
        "                                else:\n",
        "                                    all_[r] += s[r]\n",
        "\n",
        "                    for r in range(len(tolerance)):\n",
        "                        srs[r] = all_[r]\n",
        "\n",
        "                    k += 1\n",
        "\n",
        "                for r in range(len(tolerance)):\n",
        "                    if len(srs[r]) == 0:\n",
        "                        ls.append(finished[r])\n",
        "                    else:\n",
        "                        print(\"Max depth of %d reached at tolerance %.3f\" % (max_depth, tolerance[r]))\n",
        "\n",
        "        return self.display_salient(image, ls, count, filename)\n",
        "\n",
        "    def saliency_map_optim_rand(self, image, label, tolerance, debug=False, max_depth=30, filename=None):\n",
        "        \"\"\"\n",
        "        Create and then show a saliency map built with the Hierarchical Shapley method.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        label : the label with respect to which we want to analyze - typically 1\n",
        "        tolerance : the specified tolerance for a sub-region to be considered salient. A list is expected.\n",
        "        only_one_run : when False, several runs are done by also considering 16 cropped versions of the input\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        max_depth : the maximum number of divisions you want to allow before deciding the tolerance is too low.\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        ls = []\n",
        "\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        start = (0, 0)\n",
        "        end = (xf[0], xf[1])\n",
        "        size = (end[0] - start[0], end[1] - start[1])\n",
        "        lx, ly = image.shape[1], image.shape[2]\n",
        "        dx, dy = image.shape[1] // 4, image.shape[2] // 4\n",
        "\n",
        "        def salient_regions(I, sx, sy):\n",
        "\n",
        "            finished = []\n",
        "\n",
        "            for tol in tolerance:\n",
        "                k = 0\n",
        "                srs = [(start, size)]\n",
        "                current = []\n",
        "                while len(srs) > 0 and k < max_depth:\n",
        "                    all_ = []\n",
        "                    for sr in srs:\n",
        "                        s = self.do_all(I, label, sr[0], sr[1], tol, debug)\n",
        "                        if s == []:\n",
        "                            coords = np.array([sr[0][0] + sx, sr[0][1] + sy])\n",
        "                            current.append((coords, sr[1]))\n",
        "                        else:\n",
        "                            all_ += s\n",
        "                    srs = all_\n",
        "                    k += 1\n",
        "                if (k < max_depth):\n",
        "                    finished += current\n",
        "            return finished\n",
        "\n",
        "        # normal\n",
        "        a = salient_regions(image, 0, 0)\n",
        "        ls.append(a)\n",
        "        count = np.ones(image.shape)\n",
        "\n",
        "        # shifted to bottom right\n",
        "        image_br = self.background.clone()\n",
        "        image_br[:, :lx - dx, :ly - dy] = image[:, dx:, dy:]\n",
        "        ls.append(salient_regions(image_br, dx, dy))\n",
        "        count[:, dx:, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to bottom left\n",
        "        image_bl = self.background.clone()\n",
        "        image_bl[:, :lx - dx, dy:] = image[:, dx:, :ly - dy]\n",
        "        ls.append(salient_regions(image_bl, dx, -dy))\n",
        "        count[:, dx:, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top left\n",
        "        image_tl = self.background.clone()\n",
        "        image_tl[:, dx:, dy:] = image[:, :lx - dx, :ly - dy]\n",
        "        ls.append(salient_regions(image_tl, -dx, -dy))\n",
        "        count[:, :lx - dx, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top right\n",
        "        image_tr = self.background.clone()\n",
        "        image_tr[:, dx:, :ly - dy] = image[:, :lx - dx, dy:]\n",
        "        ls.append(salient_regions(image_tr, -dx, dy))\n",
        "        count[:, :lx - dx, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        return self.display_salient_optim_rand(image, ls, count, filename)\n",
        "\n",
        "    def display_salient_optim_rand(self, im, srs_coll, count, filename):\n",
        "        \"\"\"\n",
        "        Determine which of the 4 quadrants are salient, i.e. have Shapley value larger than tol\n",
        "        Parameters\n",
        "        ----------\n",
        "        im : the original image, in input format\n",
        "        srs_coll : a collection of all regions deemed salient\n",
        "        count : a normalizing mask which determines how many time each pixel was given a chance to be salient\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(60, 30))\n",
        "\n",
        "        sample_image = im.numpy().transpose(1, 2, 0)\n",
        "        count = count.transpose(1, 2, 0)\n",
        "        ax4.imshow(count / np.max(count))\n",
        "        image = sample_image * self.sd + self.mean\n",
        "        ax1.imshow(image)\n",
        "        ax2.imshow(image)\n",
        "        mask = np.zeros(image.shape)\n",
        "\n",
        "        # Count how many time each pixel was found to be in a salient region\n",
        "        for srs in srs_coll:\n",
        "            for sr in srs:\n",
        "                start = sr[0]\n",
        "                q_size = sr[1]\n",
        "\n",
        "                if (start[0] >= 0 and start[0] + q_size[0] <= image.shape[0] and start[1] >= 0 and start[1] + q_size[\n",
        "                    1] <= image.shape[1]):\n",
        "                    xs = [start[1], start[1] + q_size[1], start[1] + q_size[1], start[1]]\n",
        "                    ys = [start[0], start[0], start[0] + q_size[0], start[0] + q_size[0]]\n",
        "                    ax2.fill(xs, ys, 'r', alpha=1 / len(srs_coll))\n",
        "\n",
        "                    mask[start[0]:start[0] + q_size[0], start[1]:start[1] + q_size[1], :] += np.ones(\n",
        "                        (q_size[0], q_size[1], 3))\n",
        "\n",
        "        # Normalize the mask by the number of tries in each region\n",
        "        mask /= count\n",
        "        # Normalize the mask to the range (0,1)\n",
        "        mask /= np.max(mask)\n",
        "        # Set to 0 elements smaller than 1/5\n",
        "        negligible = (mask < 1 / 5)\n",
        "        mask[negligible] = 0\n",
        "\n",
        "        ax1.set_xlim([0, im.shape[2]])\n",
        "        ax1.set_ylim([im.shape[1], 0])\n",
        "        ax2.set_xlim([0, im.shape[2]])\n",
        "        ax2.set_ylim([im.shape[1], 0])\n",
        "        ax3.imshow(image * mask)\n",
        "        if filename != None:\n",
        "            plt.savefig(filename, dpi=300)\n",
        "        return mask\n",
        "\n",
        "    def get_list_optim_tol(self, image, label, tolerance, sx, sy, debug=False, max_depth=30):\n",
        "\n",
        "        ls = []\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        start = (0, 0)\n",
        "        end = (xf[0], xf[1])\n",
        "\n",
        "        size = (end[0] - start[0], end[1] - start[1])\n",
        "        srs = [[(start, size)] for r in range(len(tolerance))]\n",
        "        finished = [[] for r in range(len(tolerance))]\n",
        "\n",
        "        k = 0\n",
        "\n",
        "        while len(srs[0]) > 0 and k < max_depth:\n",
        "            all_ = [[] for r in range(len(tolerance))]\n",
        "\n",
        "            for sr in srs[0]:\n",
        "                s = self.do_all_optim_tol(image, label, sr[0], sr[1], tolerance, debug)\n",
        "\n",
        "                for r in range(len(tolerance)):\n",
        "                    if len(srs[r]) > 0:\n",
        "                        if s[r] == []:\n",
        "                            coords = np.array([sr[0][0] + sx, sr[0][1] + sy])\n",
        "                            finished[r].append((coords, sr[1]))\n",
        "                        else:\n",
        "                            all_[r] += s[r]\n",
        "\n",
        "            for r in range(len(tolerance)):\n",
        "                srs[r] = all_[r]\n",
        "\n",
        "            k += 1\n",
        "\n",
        "        for r in range(len(tolerance)):\n",
        "            if len(srs[r]) == 0:\n",
        "                ls.extend(finished[r])\n",
        "            else:\n",
        "                print(\"Max depth of %d reached at tolerance %.3f\" % (max_depth, tolerance[r]))\n",
        "\n",
        "        return ls\n",
        "\n",
        "    def saliency_map_optim_all(self, image, label, tolerance, debug=False, max_depth=30, filename=None):\n",
        "        \"\"\"\n",
        "        Create and then show a saliency map built with the Hierarchical Shapley method.\n",
        "        ----------\n",
        "        im : the input image\n",
        "        label : the label with respect to which we want to analyze - typically 1\n",
        "        tolerance : the specified tolerance for a sub-region to be considered salient. A list is expected.\n",
        "        only_one_run : when False, several runs are done by also considering 16 cropped versions of the input\n",
        "        debug : if True, all subsets, there associated scores and the Shapley values will be displayed\n",
        "        max_depth : the maximum number of divisions you want to allow before deciding the tolerance is too low.\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "        ls = []\n",
        "\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        start = (0, 0)\n",
        "        end = (xf[0], xf[1])\n",
        "        size = (end[0] - start[0], end[1] - start[1])\n",
        "        lx, ly = image.shape[1], image.shape[2]\n",
        "        dx, dy = image.shape[1] // 4, image.shape[2] // 4\n",
        "\n",
        "        # normal\n",
        "        a = self.get_list_optim_tol(image, label, tolerance, 0, 0, debug=False, max_depth=30)\n",
        "        ls.append(a)\n",
        "        count = np.ones(image.shape)\n",
        "\n",
        "        # shifted to bottom right\n",
        "        image_br = self.background.clone()\n",
        "        image_br[:, :lx - dx, :ly - dy] = image[:, dx:, dy:]\n",
        "        a = self.get_list_optim_tol(image_br, label, tolerance, dx, dy, debug=False, max_depth=30)\n",
        "        ls.append(a)\n",
        "        count[:, dx:, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to bottom left\n",
        "        image_bl = self.background.clone()\n",
        "        image_bl[:, :lx - dx, dy:] = image[:, dx:, :ly - dy]\n",
        "        a = self.get_list_optim_tol(image_bl, label, tolerance, dx, -dy, debug=False, max_depth=30)\n",
        "        ls.append(a)\n",
        "        count[:, dx:, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top left\n",
        "        image_tl = self.background.clone()\n",
        "        image_tl[:, dx:, dy:] = image[:, :lx - dx, :ly - dy]\n",
        "        a = self.get_list_optim_tol(image_tl, label, tolerance, -dx, -dy, debug=False, max_depth=30)\n",
        "        ls.append(a)\n",
        "        count[:, :lx - dx, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top right\n",
        "        image_tr = self.background.clone()\n",
        "        image_tr[:, dx:, :ly - dy] = image[:, :lx - dx, dy:]\n",
        "        a = self.get_list_optim_tol(image_tr, label, tolerance, -dx, dy, debug=False, max_depth=30)\n",
        "        ls.append(a)\n",
        "        count[:, :lx - dx, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "\n",
        "        return self.display_salient_optim_rand(image, ls, count, filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_k_masks(self, image, label, tolerance, debug=False, max_depth=30):\n",
        "        N_thresholds = len(tolerance)\n",
        "        \n",
        "        \n",
        "        ls = [[] for l in range(N_thresholds)]\n",
        "\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        start = (0, 0)\n",
        "        end = (xf[0], xf[1])\n",
        "        size = (end[0] - start[0], end[1] - start[1])\n",
        "        lx, ly = image.shape[1], image.shape[2]\n",
        "        dx, dy = image.shape[1] // 4, image.shape[2] // 4\n",
        "\n",
        "        # normal\n",
        "        a = self.get_finished(image, label, tolerance, 0, 0, debug=False, max_depth=30)\n",
        "        for l in range(N_thresholds): \n",
        "          ls[l].append(a[l])\n",
        "        count = np.ones(image.shape)\n",
        "\n",
        "        # shifted to bottom right\n",
        "        image_br = self.background.clone()\n",
        "        image_br[:, :lx - dx, :ly - dy] = image[:, dx:, dy:]\n",
        "        a = self.get_finished(image_br, label, tolerance, dx, dy, debug=False, max_depth=30)\n",
        "        for l in range(N_thresholds): \n",
        "          ls[l].append(a[l])\n",
        "        count[:, dx:, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to bottom left\n",
        "        image_bl = self.background.clone()\n",
        "        image_bl[:, :lx - dx, dy:] = image[:, dx:, :ly - dy]\n",
        "        a = self.get_finished(image_bl, label, tolerance, dx, -dy, debug=False, max_depth=30)\n",
        "        for l in range(N_thresholds): \n",
        "          ls[l].append(a[l])\n",
        "        count[:, dx:, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top left\n",
        "        image_tl = self.background.clone()\n",
        "        image_tl[:, dx:, dy:] = image[:, :lx - dx, :ly - dy]\n",
        "        a = self.get_finished(image_tl, label, tolerance, -dx, -dy, debug=False, max_depth=30)\n",
        "        for l in range(N_thresholds): \n",
        "          ls[l].append(a[l])\n",
        "        count[:, :lx - dx, :ly - dy] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "        # shifted to top right\n",
        "        image_tr = self.background.clone()\n",
        "        image_tr[:, dx:, :ly - dy] = image[:, :lx - dx, dy:]\n",
        "        a = self.get_finished(image_tr, label, tolerance, -dx, dy, debug=False, max_depth=30)\n",
        "        for l in range(N_thresholds): \n",
        "          ls[l].append(a[l])\n",
        "        count[:, :lx - dx, dy:] += np.ones((3, lx - dx, ly - dy))\n",
        "\n",
        "\n",
        "        masks = []\n",
        "        for l in range(N_thresholds):\n",
        "          masks.append(self.get_mask(image, ls[l], count))\n",
        "\n",
        "        return masks\n",
        "\n",
        "    def get_finished(self, image, label, tolerance, sx, sy, debug=False, max_depth=30):\n",
        "        xf = [image.shape[1], image.shape[2]]\n",
        "\n",
        "        start = (0, 0)\n",
        "        end = (xf[0], xf[1])\n",
        "\n",
        "        size = (end[0] - start[0], end[1] - start[1])\n",
        "        srs = [[(start, size)] for r in range(len(tolerance))]\n",
        "        finished = [[] for r in range(len(tolerance))]\n",
        "\n",
        "        k = 0\n",
        "\n",
        "        while len(srs[0]) > 0 and k < max_depth:\n",
        "            all_ = [[] for r in range(len(tolerance))]\n",
        "\n",
        "            for sr in srs[0]:\n",
        "                s = self.do_all_optim_tol(image, label, sr[0], sr[1], tolerance, debug)\n",
        "\n",
        "                for r in range(len(tolerance)):\n",
        "                    if len(srs[r]) > 0:\n",
        "                        if s[r] == []:\n",
        "                            coords = np.array([sr[0][0] + sx, sr[0][1] + sy])\n",
        "                            finished[r].append((coords, sr[1]))\n",
        "                        else:\n",
        "                            all_[r] += s[r]\n",
        "\n",
        "            for r in range(len(tolerance)):\n",
        "                srs[r] = all_[r]\n",
        "\n",
        "            k += 1\n",
        "\n",
        "        for r in range(len(tolerance)):\n",
        "            if len(srs[r]) != 0:\n",
        "                print(\"Max depth of %d reached at tolerance %.3f\" % (max_depth, tolerance[r]))\n",
        "\n",
        "        return finished\n",
        "\n",
        "    def get_mask(self, im, srs_coll, count):\n",
        "        \"\"\"\n",
        "        Determine which of the 4 quadrants are salient, i.e. have Shapley value larger than tol\n",
        "        Parameters\n",
        "        ----------\n",
        "        im : the original image, in input format\n",
        "        srs_coll : a collection of all regions deemed salient\n",
        "        count : a normalizing mask which determines how many time each pixel was given a chance to be salient\n",
        "        filename : name of the file to save the figure to\n",
        "        \"\"\"\n",
        "\n",
        "        count = count.transpose(1, 2, 0)\n",
        "        mask = np.zeros(count.shape)\n",
        "        # Count how many time each pixel was found to be in a salient region\n",
        "        for srs in srs_coll:\n",
        "            for sr in srs:\n",
        "                start = sr[0]\n",
        "                q_size = sr[1]\n",
        "\n",
        "                if (start[0] >= 0 and start[0] + q_size[0] <= count.shape[0] and start[1] >= 0 and start[1] + q_size[\n",
        "                    1] <= count.shape[1]):\n",
        "                    mask[start[0]:start[0] + q_size[0], start[1]:start[1] + q_size[1], :] += np.ones(\n",
        "                        (q_size[0], q_size[1], 3))\n",
        "\n",
        "        # Normalize the mask by the number of tries in each region\n",
        "        mask /= count\n",
        "        # Normalize the mask to the range (0,1)\n",
        "        mask /= np.max(mask)\n",
        "        # Set to 0 elements smaller than 1/5\n",
        "        negligible = (mask < 1 / 5)\n",
        "        mask[negligible] = 0\n",
        "        return mask\n",
        "\n",
        "\n",
        "average = torch.mean(X, dim = 0)\n",
        "h = HierarchicalShap(net, background = average)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Requirement already satisfied: flashtorch in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.18.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from flashtorch) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from flashtorch) (1.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from flashtorch) (3.2.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from flashtorch) (0.5.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources->flashtorch) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources->flashtorch) (3.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->flashtorch) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->flashtorch) (1.12.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.6/dist-packages (0.35.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.18.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.38.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (0.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->shap) (1.12.0)\n",
            "Requirement already satisfied: pytorch-gradcam in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (1.18.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from pytorch-gradcam) (4.1.2.30)\n",
            "File local_modules/cam.py already there; not retrieving.\n",
            "\n",
            "File local_modules/visualize.py already there; not retrieving.\n",
            "\n",
            "File local_modules/HierarchicalShapley.py already there; not retrieving.\n",
            "\n",
            "File local_modules/utils.py already there; not retrieving.\n",
            "\n",
            "File local_modules/interpret11.py already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_e9BA6GvwJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9fdfe744-d2e7-4ad4-e2b1-7915491cc207"
      },
      "source": [
        "masks = h.get_k_masks(image, label, tolerance = [8,9,10,11,12], debug=False, max_depth = 30)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max depth of 30 reached at tolerance 8.000\n",
            "Max depth of 30 reached at tolerance 9.000\n",
            "Max depth of 30 reached at tolerance 10.000\n",
            "Max depth of 30 reached at tolerance 11.000\n",
            "Max depth of 30 reached at tolerance 8.000\n",
            "Max depth of 30 reached at tolerance 9.000\n",
            "Max depth of 30 reached at tolerance 8.000\n",
            "Max depth of 30 reached at tolerance 9.000\n",
            "Max depth of 30 reached at tolerance 8.000\n",
            "Max depth of 30 reached at tolerance 8.000\n",
            "Max depth of 30 reached at tolerance 9.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTpMm0X4xzh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ImF = ImageFolder(root = \"/content/drive/My Drive/Interpretability/img4saliencymap/dataHD\", transform = transf)\n",
        "batch_Size = 100\n",
        "exloader = DataLoader(ImF, batch_size = batch_Size, shuffle = False, num_workers = 0)\n",
        "\n",
        "exIter = iter(exloader)\n",
        "images, labels = next(exIter)\n",
        "\n",
        "def shap_exp(e, inp, img):\n",
        "    shapley_values, indexes = e.shap_values(inp, ranked_outputs=2, nsamples=125)\n",
        "    \n",
        "    shapley_values = [np.swapaxes(np.swapaxes(s, 2, 3), 1, -1) for s in shapley_values ]\n",
        "\n",
        "    image = img[np.newaxis, :]\n",
        "\n",
        "    shap.image_plot(shapley_values, image, indexes.numpy(), show=False, width = 100)\n",
        "    plt.suptitle(\"All channels together\");\n",
        "    plt.savefig('fig1.png', dpi = 300)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNnMrlJOaOIC",
        "colab_type": "code",
        "outputId": "b3c4196f-a6d2-4219-d3ea-e50944ba3ac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "i = 2\n",
        "\n",
        "image = images[i]\n",
        "label = labels[i]\n",
        "input_ = image.view(-1, 3, 1000, 1200) # This shape is necessary for the network \n",
        "output = net(input_)\n",
        "_, predicted = torch.max(output.data, 1)\n",
        "\n",
        "img = utils.input2image(image, MEAN, STD)\n",
        "\n",
        "shap_exp(e, input_.detach(), img)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x360 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKHXvTypS8kZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contours_from_mask_shap(M, show_thresholded):\n",
        "  M_int = (255*M).astype(np.uint8)\n",
        "  blurred = cv2.blur(M_int,(10,10))\n",
        "  gray_image = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
        "  ret,thresh = cv2.threshold(gray_image,0,255,0)\n",
        "  print(np.max(thresh),np.min(thresh),np.mean(thresh))\n",
        "  if show_thresholded:\n",
        "    f, ax = plt.subplots(figsize = (10,10))\n",
        "    plt.imshow(thresh, cmap = 'gray')\n",
        "  contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  return contours\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHOFFN4Ih4dB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def contours_from_mask(M, show_thresholded):\n",
        "  M_int = (255*M).astype(np.uint8)\n",
        "  blurred = cv2.blur(M_int,(10,10))\n",
        "  gray_image = cv2.cvtColor(blurred, cv2.COLOR_BGR2GRAY)\n",
        "  ret,thresh = cv2.threshold(gray_image,0,255,0)\n",
        "  if show_thresholded:\n",
        "    f, ax = plt.subplots(figsize = (10,10))\n",
        "    plt.imshow(thresh, cmap = 'gray')\n",
        "  contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "  return contours\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2xwkc0FFMvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def compare_contours_positions(img, contours, positions):\n",
        "  f, axs = plt.subplots(1,3, figsize = (24,8))\n",
        "  axs[0].imshow(cv2.drawContours(np.zeros(img.shape), contours, -1, (255,0,0), 3).astype(np.uint8))\n",
        "  axs[1].plot(positions[:,0], 1000-positions[:,1],'*')\n",
        "  axs[1].set_ylim([0,1000])\n",
        "  axs[1].set_xlim([0,1200])\n",
        "  axs[2].imshow(cv2.drawContours(np.zeros(img.shape), contours, -1, (0,255,0), 3).astype(np.uint8))\n",
        "  axs[2].plot(positions[:,0], positions[:,1], '*')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl0IPj2jFyb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def judge_predictions_contours(positions, contours): \n",
        "  is_within = np.zeros((len(positions), len(contours)))\n",
        "  for i in range(len(positions)): \n",
        "    for j in range(len(contours)): \n",
        "      is_within[i,j] = (cv2.pointPolygonTest(contours[j], (positions[i][0], positions[i][1]), False) >= 0)\n",
        "  n_pred_per_point = np.sum(is_within, axis = 1) # How many times each real point was detected\n",
        "  n_correct_predictions = (n_pred_per_point ==1).sum()\n",
        "  multiple_pred = (n_pred_per_point > 1).sum()\n",
        "  not_detected = (n_pred_per_point == 0).sum()\n",
        "  invented_salient = (np.sum(is_within, axis = 0) ==0).sum()\n",
        "  return (len(positions), n_correct_predictions, not_detected, multiple_pred, invented_salient)\n",
        "  \n",
        "def judge_predictions_centroids(positions, predictions): \n",
        "  distances = np.zeros((len(positions), len(predictions)))\n",
        "  for i in range(len(positions)): \n",
        "    for j in range(len(predictions)): \n",
        "      d2 = (positions[i,0]-predictions[j,0])**2 + (positions[i,1]-predictions[j,1])**2\n",
        "      distances[i,j] = (d2 < 150)\n",
        "  n_pred_per_point = np.sum(distances, axis = 1) # How many times each real point was detected\n",
        "  n_correct_predictions = (n_pred_per_point ==1).sum()\n",
        "  multiple_pred = (n_pred_per_point > 1).sum()\n",
        "  not_detected = (n_pred_per_point == 0).sum()\n",
        "  invented_salient = (np.sum(distances, axis = 0) ==0).sum()\n",
        "\n",
        "  return (len(positions), n_correct_predictions, not_detected, multiple_pred, invented_salient)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yClBy4nI70MU",
        "colab_type": "code",
        "outputId": "ec2a1ab5-ab05-4213-ef80-e5c85399035c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "tols = [0, 0.1, 1, 10, 100, 1000]\n",
        "total = [0 for i in range(len(tols))]\n",
        "true_pos = [0 for i in range(len(tols))]\n",
        "not_detected = [0 for i in range(len(tols))]\n",
        "multiple_detection = [0 for i in range(len(tols))]\n",
        "false_pos = [0 for i in range(len(tols))]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(2): \n",
        "  print(\"/n /n ROUND %d\" %i)\n",
        "  image = images[i]\n",
        "  label = labels[i]\n",
        "  img = utils.input2image(image, MEAN, STD)\n",
        "  masks = h.get_k_masks(image, label, tolerance = tols, debug=False, max_depth = 30)\n",
        "  filename = \"/content/drive/My Drive/Interpretability/img4saliencymap/dataHD/1/positions/positions_\" + str(i).zfill(5) + \".csv\"\n",
        "  positions = pd.read_csv(filename, header = None).values\n",
        "  for k in range(len(tols)): \n",
        "    contours = contours_from_mask(masks[k], False)\n",
        "    N, TP, NF, mult, FP = judge_predictions_contours(positions, contours)\n",
        "    total[k] += N\n",
        "    true_pos[k] += TP\n",
        "    not_detected[k] += NF\n",
        "    multiple_detection[k] += mult \n",
        "    false_pos[k] += FP \n",
        "\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/n /n ROUND 0\n",
            "Max depth of 30 reached at tolerance 0.000\n",
            "Max depth of 30 reached at tolerance 0.100\n",
            "Max depth of 30 reached at tolerance 0.000\n",
            "Max depth of 30 reached at tolerance 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-e0d9bb74254b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcontours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontours_from_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcompare_contours_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjudge_predictions_contours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-0f9132ff6dce>\u001b[0m in \u001b[0;36mcompare_contours_positions\u001b[0;34m(img, contours, positions)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompare_contours_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/core/src/matrix_c.cpp:185: error: (-5:Bad argument) Unknown array type in function 'cvarrToMat'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1728x576 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQt3AxexFk0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "20bab913-43d6-4a3b-d9a4-a95ada54ceb8"
      },
      "source": [
        "for k in range(len(tols)): \n",
        "  print(tols[k], \": \",total[k], true_pos[k], not_detected[k], multiple_detection[k], false_pos[k], 100*(true_pos[k]/total[k]))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 :  72 72 0 0 73 100.0\n",
            "0.1 :  72 72 0 0 18 100.0\n",
            "1 :  72 72 0 0 21 100.0\n",
            "10 :  72 72 0 0 21 100.0\n",
            "100 :  72 72 0 0 0 100.0\n",
            "1000 :  72 72 0 0 0 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}